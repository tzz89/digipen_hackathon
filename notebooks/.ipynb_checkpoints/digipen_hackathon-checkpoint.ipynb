{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0460220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gc\n",
    "import pickle\n",
    "import lightgbm\n",
    "\n",
    "#visualization\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#datacleaning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler,PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#models\n",
    "from sklearn.model_selection import GridSearchCV,cross_validate,ParameterGrid,train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from xgboost import XGBClassifier \n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Version control\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682bbcc5",
   "metadata": {},
   "source": [
    "## Main config file\n",
    "This config file will be transformed intot a config.yaml file during production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a078b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    random_seed = 43,\n",
    "    train_filepath = os.path.join(\"..\", \"data\", \"raw_data\" ,\"Train.csv\"),\n",
    "    test_filepath  = os.path.join(\"..\", \"data\", \"raw_data\" ,\"Test.csv\"),\n",
    "    label_map = dict(PASS=1, FAIL=0),\n",
    "    fig_root = os.path.join(\"..\", \"figures\"),\n",
    "    \n",
    "    normalizers = {\n",
    "        \"normalizer\":[MinMaxScaler(),StandardScaler(),RobustScaler()]\n",
    "    },\n",
    "    \n",
    "    models = [dict(estimator=RandomForestClassifier(random_state=43),\n",
    "                   name = \"RandomForest\",\n",
    "                   params={\"clf__n_estimators\":[70,75,85]}),\n",
    "              \n",
    "#              dict(estimator=LinearSVC(),\n",
    "#                   name = \"LinearSVC\",\n",
    "#                   params={\"clf__C\":[0.5,1.0,1.2]}),\n",
    "              \n",
    "             dict(estimator=SVC(),\n",
    "                  name = \"SVC\",\n",
    "                  params={\"clf__C\":[0.7,0.8,0.9],\n",
    "                          \"clf__kernel\":['poly','rbf']}),\n",
    "              \n",
    "             dict(estimator=XGBClassifier(use_label_encoder=False),\n",
    "                  name = \"XGBClassifier\",\n",
    "                  params={\"clf__n_estimators\":[90,100,110],\n",
    "                          \"clf__max_depth\":[3,4,5],\n",
    "                          \"clf__eval_metric\":['mlogloss'],\n",
    "                          }),\n",
    "              \n",
    "             dict(estimator=LGBMClassifier(),\n",
    "                  name = \"LGBMClassifier\",\n",
    "                  params={\"clf__n_estimators\":[70,80,90],\n",
    "                          \"clf__num_leaves\":[16,17,18]}),\n",
    "             ],\n",
    "    \n",
    "    scorings = [\"f1\",\"accuracy\",\"f1_micro\",\n",
    "                \"f1_macro\",\"f1_weighted\", \"roc_auc\",\n",
    "                \"precision\",\"recall\"],\n",
    "    \n",
    "    trained_model_dir = os.path.join(\"..\", \"models\",\"trained_model.pkl\"),\n",
    "    \n",
    "    prediction_file_path = os.path.join(\"..\", \"data\", \"raw_data\" ,\"test_prediction.csv\"),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcdd34",
   "metadata": {},
   "source": [
    "## Utility and custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformToNumeric(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    This class serve 2 purposes. \n",
    "    1. Ensure correct conversion of \"O\" type to float64\n",
    "    2. Only extract the features that are  present in the training features to continue to down stream processes\n",
    "    ''' \n",
    "    def __init__(self):\n",
    "        self.columns_to_transfrom =[]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.columns_to_transfrom = X.columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X[self.columns_to_transfrom].copy()\n",
    "        for col in X_copy.columns:\n",
    "            try:\n",
    "                X_copy[col] = X_copy[col].astype(np.float64)\n",
    "            except:\n",
    "                X_copy[col] = pd.to_numeric(X_copy[col],errors=\"coerce\").values\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class PolyFeatures(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, columns_to_transform):\n",
    "        self.columns_to_transform= columns_to_transform\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y= None):\n",
    "        df = df.copy()\n",
    "        for i in range(len(self.columns_to_transform)-1):\n",
    "            for j in range(i+1,len(self.columns_to_transform)):\n",
    "                column_name = \"{}_{}\".format(self.columns_to_transform[i],self.columns_to_transform[j])\n",
    "                df[column_name] = df[self.columns_to_transform[i]]*df[self.columns_to_transform[j]]\n",
    "#                 print(column_name)\n",
    "        return df\n",
    "    \n",
    "class LabelMapper():\n",
    "    def __init__(self,label_map):\n",
    "        self.label_map = label_map\n",
    "        \n",
    "    def __call__(self,row):\n",
    "        return self.label_map[row]\n",
    "    \n",
    "    def reverse_map(self, row):\n",
    "        reverse_mapping = {value:key for key, value in selt.label_map.items()}\n",
    "        return reverse_mapping[row]\n",
    "    \n",
    "def get_best_model(train_models):\n",
    "    best_score= -1\n",
    "    best_model = None\n",
    "    all_scores = []\n",
    "    \n",
    "    \n",
    "    for train_model in train_models:\n",
    "        model_best_score = train_model.best_score_\n",
    "        all_scores.append(model_best_score)\n",
    "        if model_best_score > best_score:\n",
    "            best_score = model_best_score\n",
    "            best_model = train_model\n",
    "        \n",
    "    print(\"Best score of {:.2f} among {}\".format(best_score*100, all_scores))\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ffc60",
   "metadata": {},
   "source": [
    "## Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1177300",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a7c19",
   "metadata": {},
   "source": [
    "## Loading of data and changing of data into correct datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "train_df = pd.read_csv(CONFIG['train_filepath'],index_col=0)\n",
    "test_df  = pd.read_csv(CONFIG['test_filepath'], index_col=0)\n",
    "\n",
    "#split training data into features and labels\n",
    "X_train = train_df.drop(\"STATUS\",axis=1)\n",
    "y_train = train_df[\"STATUS\"]\n",
    "\n",
    "#Converting X_train and test_df into float32/nan\n",
    "data_type_transformer = TransformToNumeric()\n",
    "X_train = data_type_transformer.fit_transform(X_train)\n",
    "test_df = data_type_transformer.transform(test_df)\n",
    "\n",
    "#mapping of y_train data into numeric \n",
    "laber_mapper = LabelMapper(CONFIG['label_map'])\n",
    "y_train = y_train.map(laber_mapper)\n",
    "\n",
    "#removing rows with more than 50% missing data\n",
    "train_df = pd.concat([X_train, y_train], axis = 1)\n",
    "print(\"Number of training data before removing removing rows with more than 50% missing data:\", len(train_df) )\n",
    "train_df = train_df.dropna(thresh=len(train_df.columns)//2)\n",
    "print(\"Number of training data after removing removing rows with more than 50% missing data:\", len(train_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split training data into features and labels\n",
    "X_train = train_df.drop(\"STATUS\",axis=1)\n",
    "y_train = train_df[\"STATUS\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.1,random_state=CONFIG[\"random_seed\"],stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcf371",
   "metadata": {},
   "source": [
    "## EDA\n",
    "This is the exploration phase. We do this to understand the data\n",
    "1. Histogram plot\n",
    "2. crossplot\n",
    "3. Number of missing data\n",
    "4. Outliers\n",
    "6. Check the missing data row distribution, use seaborn to plot\n",
    "5. **Check wif the distribution of the test file is the same as the training set (not practical as we will optimized for purely unseen data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = train_df['STATUS'].value_counts()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=label_counts.values, x=label_counts.index,\n",
    "                     text=label_counts.values*100/sum(label_counts.values),\n",
    "                     textposition='auto'))\n",
    "fig.update_layout(title=\"Label distribution\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking of missing data heat map\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\n",
    "\n",
    "sns.heatmap(train_df.isna(),ax=ax[0])\n",
    "sns.heatmap(test_df.isna(),ax=ax[1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X_train_na_cols = {}\n",
    "test_na_cols = {}\n",
    "\n",
    "for col in X_train.columns:\n",
    "    num_missing_value = X_train[col].isna().sum()\n",
    "    total_row = len(X_train)\n",
    "    if num_missing_value > 4:\n",
    "        X_train_na_cols[col] = num_missing_value/total_row *100\n",
    "        \n",
    "for col in test_df.columns:\n",
    "    num_missing_value = test_df[col].isna().sum()\n",
    "    total_row = len(test_df)\n",
    "    if num_missing_value > 3:\n",
    "        test_na_cols[col] = num_missing_value/total_row *100\n",
    "        \n",
    "        \n",
    "train_missing_df = pd.DataFrame(X_train_na_cols,index=[\"Train_data\"])\n",
    "test_missing_df = pd.DataFrame(test_na_cols,index=[\"Test_data\"])\n",
    "test_missing_df.append(train_missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d848f",
   "metadata": {},
   "source": [
    "### We can see that the number of missing value is not a huge percentage of the total number of the observation. Since the training set is 4 times as large as the test set, we set the missing value threshold to 4. We see this special column that we may choose to dive deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152495b",
   "metadata": {},
   "source": [
    "## Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deed3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the violin plot for train and test data\n",
    "fig = make_subplots(rows=42, cols=4, subplot_titles=X_train.columns)\n",
    "fig.update_layout(height=12000, width=1200, title_text=\"Distribution plot\")\n",
    "\n",
    "for i, col_name in enumerate(X_train.columns):\n",
    "    \n",
    "    row = (i // 4) + 1\n",
    "    col = (i %  4) + 1 \n",
    "    \n",
    "    fig.add_trace(go.Violin(y=X_train[col_name], box_visible=True,\n",
    "                  meanline_visible=True, opacity=0.6, name=\"TRAIN\", showlegend=False, \n",
    "                            points=\"outliers\",pointpos=-1), row=row, col= col,)\n",
    "\n",
    "    fig.add_trace(go.Violin(y=test_df[col_name], box_visible=True,\n",
    "                  meanline_visible=True, opacity=0.6, name=\"TEST\", showlegend=False, \n",
    "                            points=\"outliers\",pointpos=1), row=row, col= col)\n",
    "\n",
    "fig.write_html(os.path.join(CONFIG['fig_root'],\"raw_violin_plot.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b7c22",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "sns.heatmap(X_train.corr(), ax= ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bf506",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "1. Remove zero variance columns\n",
    "2. Remove outlier (not going to remove outlier as data does not support removing any outliers)\n",
    "3. Normalization #have to come before imputation\n",
    "4. Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing zero variance removal\n",
    "zero_variance_removal= VarianceThreshold(threshold=0.0)\n",
    "print(\"Number of columns before removing zero variance cols\", len(X_train.columns))\n",
    "after_remove = zero_variance_removal.fit_transform(X_train)\n",
    "print(\"Number of columns after removing zero variance cols\", sum(zero_variance_removal.get_support()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddb479",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bcc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "\n",
    "normalizers = CONFIG['normalizers']\n",
    "\n",
    "for model in CONFIG[\"models\"]:\n",
    "    model_obj = model[\"estimator\"]\n",
    "    model_name = model['name']\n",
    "    model_params = model[\"params\"]\n",
    "    \n",
    "   \n",
    "    \n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        (\"feature_selector\", VarianceThreshold()),\n",
    "        (\"normalizer\",StandardScaler()),\n",
    "        (\"imputer\",SimpleImputer()),\n",
    "        (\"clf\", model_obj)\n",
    "    ])\n",
    "    param_grid = {**normalizers, **model_params }\n",
    "    \n",
    "    print(\"Running model:\",model_obj, \"Model name:\",model_name)\n",
    "    print(\"model_params:\", param_grid)\n",
    "    \n",
    "    \n",
    "    search_model = GridSearchCV(model_pipeline,param_grid=param_grid,cv=5,scoring=CONFIG['scorings'],refit=\"f1_macro\")\n",
    "    search_model.fit(X_train,y_train)\n",
    "    trained_models.append(search_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad7d56",
   "metadata": {},
   "source": [
    "## Saving the trained model and loading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500861d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG[\"trained_model_dir\"] , 'wb') as handles:\n",
    "    pickle.dump(trained_models, handles, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(CONFIG[\"trained_model_dir\"] , 'rb') as handles:\n",
    "    trained_models = pickle.load(handles)\n",
    "    \n",
    "best_model = get_best_model(trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91deb9b8",
   "metadata": {},
   "source": [
    "## Validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(X_val)\n",
    "print(classification_report(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c22a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = best_model.best_estimator_['feature_selector']\n",
    "new_columns = X_train.columns[feature_selector.get_support()]\n",
    "new_columns[76]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c595719",
   "metadata": {},
   "source": [
    "## Plotting feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c257a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model = best_model.best_estimator_['clf']\n",
    "lightgbm_model\n",
    "lightgbm.plot_importance(lightgbm_model,figsize=(20,30));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83904506",
   "metadata": {},
   "source": [
    "## Feature cross attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y = X_train['C94']**0.5, x= X_train['C3'],mode='markers',marker=dict(color=y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f00a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "features_to_enhance =['C94', 'C3', 'C5', 'C2', 'C17']\n",
    "\n",
    "normalizers = CONFIG['normalizers']\n",
    "\n",
    "for model in CONFIG[\"models\"]:\n",
    "    model_obj = model[\"estimator\"]\n",
    "    model_name = model['name']\n",
    "    model_params = model[\"params\"]\n",
    "    \n",
    "   \n",
    "    \n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        (\"feature_enchance\", PolyFeatures(features_to_enhance)),\n",
    "        (\"feature_selector\", VarianceThreshold()),\n",
    "        (\"normalizer\",StandardScaler()),\n",
    "        (\"imputer\",SimpleImputer()),\n",
    "        (\"clf\", model_obj)\n",
    "    ])\n",
    "    param_grid = {**normalizers, **model_params }\n",
    "    \n",
    "    print(\"Running model:\",model_obj, \"Model name:\",model_name)\n",
    "    print(\"model_params:\", param_grid)\n",
    "    \n",
    "    \n",
    "    search_model = GridSearchCV(model_pipeline,param_grid=param_grid,cv=5,scoring=CONFIG['scorings'],refit=\"f1_macro\")\n",
    "    search_model.fit(X_train,y_train)\n",
    "    trained_models.append(search_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d516ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG[\"trained_model_dir\"] , 'wb') as handles:\n",
    "    pickle.dump(trained_models, handles, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(CONFIG[\"trained_model_dir\"] , 'rb') as handles:\n",
    "    trained_models = pickle.load(handles)\n",
    "    \n",
    "best_model = get_best_model(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(X_val)\n",
    "print(classification_report(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aece99",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb023a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = best_model.predict(test_df)\n",
    "test_df['prediction'] = test_prediction\n",
    "test_df.to_csv(CONFIG['prediction_file_path'])\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
